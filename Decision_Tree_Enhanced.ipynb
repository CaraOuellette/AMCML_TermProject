{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b452251-7348-41cc-a7f9-e88f50ce76cc",
   "metadata": {},
   "outputs": [],
   "source": "# 1. Imports and Configuration\n\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split, GridSearchCV, TimeSeriesSplit\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn.metrics import (\n    accuracy_score,\n    classification_report,\n    confusion_matrix,\n    f1_score,\n    roc_auc_score,\n    average_precision_score,\n    make_scorer\n)\nfrom sklearn.preprocessing import StandardScaler\n\n# For nice plots\nsns.set(style=\"whitegrid\", context=\"notebook\")\n\n# Configuration\nDATE_COL = \"date\"\nSPLIT_DATE = \"2022-01-01\"\nRANDOM_STATE = 42\n\n# Load dataset\ndf = pd.read_csv(\"master_dataset_ml_ready_labelled.csv\")\ndf[DATE_COL] = pd.to_datetime(df[DATE_COL], errors='coerce')\ndf = df.sort_values(by=DATE_COL).reset_index(drop=True)\n\n# Dataset description\ndescription = \"\"\"\n### Enhanced Decision Tree Notebook\nChanges from original:\n- Binary target: PriceUp (1 if next day's price increases)\n- Engineered features: curve_spread, storage_change, momentum indicators\n- Chronological train/test split at 2022-01-01\n- TimeSeriesSplit cross-validation\n- Multiple metrics: Accuracy, F1, ROC-AUC, PR-AUC\n\"\"\"\nprint(description)\ndf.head()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7347c3-f168-4c02-b95d-1dd15cc9a92b",
   "metadata": {},
   "outputs": [],
   "source": "# 2. Feature Engineering & Dataset Description\n\n# --- Feature Engineering ---\n# 1. Target: Binary Next-Day Direction (PriceUp)\ndf['return'] = df['spot_price'].pct_change()\ndf['PriceUp'] = (df['return'].shift(-1) > 0).astype(int)\n\n# 2. Synthetic Features\n# A. Curve Spread (Futures curve shape)\ndf['curve_spread'] = df['contract_2_price'] - df['contract_1_price']\n\n# B. Storage Change (7-day delta)\ndf['storage_bcf_change_7d'] = df['storage_bcf'].diff(7)\n\n# C. Aggregated Weather\nhdd_cols = [c for c in df.columns if c.startswith('HDD_')]\ncdd_cols = [c for c in df.columns if c.startswith('CDD_')]\n\ndf['HDD_total'] = df[hdd_cols].mean(axis=1)\ndf['CDD_total'] = df[cdd_cols].mean(axis=1)\ndf['net_weather'] = df['HDD_total'] - df['CDD_total']\n\n# D. Momentum / Returns Features\ndf['ret_1'] = df['return'].shift(1)       # Yesterday's return\ndf['ret_3'] = df['return'].rolling(3).mean()\ndf['ret_5'] = df['return'].rolling(5).mean()\ndf['ret_10'] = df['return'].rolling(10).mean()\n\n# 3. Cleanup - Drop rows with NaNs created by lags/rolling/target shift\ninitial_shape = df.shape\ndf.dropna(inplace=True)\nprint(f\"Dropped {initial_shape[0] - df.shape[0]} rows due to lag/rolling NaN creation.\")\n\nprint(\"\\nShape (rows, columns):\", df.shape)\nprint(\"\\nNew columns added:\", ['curve_spread', 'storage_bcf_change_7d', 'HDD_total', \n                               'CDD_total', 'net_weather', 'ret_1', 'ret_3', 'ret_5', 'ret_10'])\n\nprint(\"\\nBinary Target Distribution (PriceUp):\")\nprint(df[\"PriceUp\"].value_counts())\nprint(df[\"PriceUp\"].value_counts(normalize=True))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d5316b-6192-45e2-a750-07fb8dc96338",
   "metadata": {},
   "outputs": [],
   "source": "# 3.1 Verify no missing values remain\n\nprint(\"Missing values per column after feature engineering cleanup:\")\nmissing = df.isna().sum()\nprint(missing[missing > 0] if missing.sum() > 0 else \"No missing values!\")\n\nprint(\"\\nFinal shape:\", df.shape)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707652e6-d17f-43f9-970d-7fa39c3777d6",
   "metadata": {},
   "outputs": [],
   "source": "# 3.2 Define features and target (Binary: PriceUp)\n\n# Columns to drop (leakage and non-predictive)\ncols_to_drop = [\n    DATE_COL, \n    'PriceUp',\n    'spot_price',  # Drop to avoid leakage (target derived from this)\n    'price_movement_scaled', \n    'price_movement_raw', \n    'return'  # This is basically the target, just unshifted\n]\n\nX = df.drop(columns=cols_to_drop, errors='ignore')\ny = df['PriceUp']\n\n# Ensure X is strictly numeric\nX = X.select_dtypes(include=[np.number])\n\nprint(f\"Feature columns ({X.shape[1]} features):\")\nprint(X.columns.tolist())\nprint(\"\\nX shape:\", X.shape)\nprint(\"y shape:\", y.shape)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdef37f4-e8a4-49a1-8638-9bfd2c2d6e1f",
   "metadata": {},
   "outputs": [],
   "source": "# 3.3 Chronological Trainâ€“Test Split (proper for time-series)\n\n# Chronological Split at 2022-01-01\nmask_train = df[DATE_COL] < SPLIT_DATE\nmask_test = df[DATE_COL] >= SPLIT_DATE\n\nX_train, X_test = X[mask_train], X[mask_test]\ny_train, y_test = y[mask_train], y[mask_test]\n\nprint(f\"Train Dates: < {SPLIT_DATE} | Shape: {X_train.shape}\")\nprint(f\"Test Dates: >= {SPLIT_DATE} | Shape: {X_test.shape}\")\n\nprint(f\"\\nClass distribution in training set:\")\nprint(y_train.value_counts(normalize=True))\nprint(f\"\\nClass distribution in test set:\")\nprint(y_test.value_counts(normalize=True))\n\n# Note: Decision Trees don't require scaling, but we keep it available\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9993467e-35f8-4ebe-9995-77d11a5f0deb",
   "metadata": {},
   "outputs": [],
   "source": "# 4. Baseline Decision Tree (with multi-metric evaluation)\n\ndt_baseline = DecisionTreeClassifier(\n    random_state=RANDOM_STATE,\n    class_weight=\"balanced\"  # handle class imbalance\n)\n\ndt_baseline.fit(X_train, y_train)\n\ny_pred_baseline = dt_baseline.predict(X_test)\ny_probs_baseline = dt_baseline.predict_proba(X_test)[:, 1]\n\n# Multi-metric evaluation\nbaseline_acc = accuracy_score(y_test, y_pred_baseline)\nbaseline_f1 = f1_score(y_test, y_pred_baseline, pos_label=1)\nbaseline_roc = roc_auc_score(y_test, y_probs_baseline)\nbaseline_pr = average_precision_score(y_test, y_probs_baseline)\n\nprint(\"=== Baseline Decision Tree Results ===\")\nprint(f\"Accuracy:          {baseline_acc:.4f}\")\nprint(f\"F1 Score (Up):     {baseline_f1:.4f}\")\nprint(f\"ROC-AUC Score:     {baseline_roc:.4f}\")\nprint(f\"PR-AUC Score:      {baseline_pr:.4f}\")\n\nprint(\"\\nClassification report (baseline):\")\nprint(classification_report(y_test, y_pred_baseline, target_names=['Down (0)', 'Up (1)']))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c14909-4bf4-4457-83b4-58f57e3b7b55",
   "metadata": {},
   "outputs": [],
   "source": "# Baseline Confusion Matrix (Binary)\ncm_baseline = confusion_matrix(y_test, y_pred_baseline, labels=[0, 1])\n\nplt.figure(figsize=(6, 4))\nsns.heatmap(cm_baseline, annot=True, fmt=\"d\", cmap=\"Blues\",\n            xticklabels=['Down (0)', 'Up (1)'],\n            yticklabels=['Down (0)', 'Up (1)'])\nplt.title(\"Baseline Decision Tree - Confusion Matrix\")\nplt.ylabel('Actual')\nplt.xlabel('Predicted')\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5c3aac-d387-416c-b76a-00df3fafc934",
   "metadata": {},
   "outputs": [],
   "source": "# 5.1 GridSearchCV with TimeSeriesSplit (optimizing F1 score)\n\n# Use TimeSeriesSplit for proper time-series cross-validation\ntscv = TimeSeriesSplit(n_splits=3)  # 3 folds for efficiency\n\nparam_grid_dt = {\n    \"max_depth\": [3, 5, 7, 10, None],\n    \"min_samples_split\": [2, 5, 10, 20],\n    \"min_samples_leaf\": [1, 2, 4, 8],\n    \"class_weight\": [None, \"balanced\", {0: 1.0, 1: 2.0}, {0: 1.0, 1: 2.5}]\n}\n\n# Use F1 score for minority class as optimization metric\nf1_scorer = make_scorer(f1_score, pos_label=1)\n\ndt = DecisionTreeClassifier(random_state=RANDOM_STATE)\n\nprint(\"--- Tuning Decision Tree via GridSearchCV (TimeSeriesSplit) ---\")\nn_combos = 5 * 4 * 4 * 4\nprint(f\"Parameter combinations: {n_combos}\")\nprint(f\"With {tscv.n_splits} CV folds = {n_combos * tscv.n_splits} model fits\\n\")\n\ngrid_dt = GridSearchCV(\n    estimator=dt,\n    param_grid=param_grid_dt,\n    scoring=f1_scorer,  # Optimize for F1 (minority class)\n    cv=tscv,\n    n_jobs=-1,\n    verbose=1,\n    refit=True\n)\n\ngrid_dt.fit(X_train, y_train)\n\nprint(f\"\\nBest parameters: {grid_dt.best_params_}\")\nprint(f\"Best CV F1 Score: {grid_dt.best_score_:.4f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40edb63-d65e-4cb6-ac45-77269d86b8c6",
   "metadata": {},
   "outputs": [],
   "source": "# 5.2 Evaluate tuned tree on test set (multi-metric)\n\ndt_best = grid_dt.best_estimator_\n\ny_pred_tuned = dt_best.predict(X_test)\ny_probs_tuned = dt_best.predict_proba(X_test)[:, 1]\n\n# Multi-metric evaluation\ntuned_acc = accuracy_score(y_test, y_pred_tuned)\ntuned_f1 = f1_score(y_test, y_pred_tuned, pos_label=1)\ntuned_roc = roc_auc_score(y_test, y_probs_tuned)\ntuned_pr = average_precision_score(y_test, y_probs_tuned)\n\nprint(\"=== Tuned Decision Tree Results (Test Set) ===\")\nprint(f\"Accuracy:          {tuned_acc:.4f}\")\nprint(f\"F1 Score (Up):     {tuned_f1:.4f}\")\nprint(f\"ROC-AUC Score:     {tuned_roc:.4f}\")\nprint(f\"PR-AUC Score:      {tuned_pr:.4f}\")\n\nprint(\"\\nClassification report (tuned):\")\nprint(classification_report(y_test, y_pred_tuned, target_names=['Down (0)', 'Up (1)']))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136415a2-aaf2-464e-b672-097612ad12c6",
   "metadata": {},
   "outputs": [],
   "source": "# Tuned Confusion Matrix (Binary)\ncm_tuned = confusion_matrix(y_test, y_pred_tuned, labels=[0, 1])\n\nplt.figure(figsize=(6, 4))\nsns.heatmap(cm_tuned, annot=True, fmt=\"d\", cmap=\"Greens\",\n            xticklabels=['Down (0)', 'Up (1)'],\n            yticklabels=['Down (0)', 'Up (1)'])\nplt.title(\"Tuned Decision Tree - Confusion Matrix\")\nplt.ylabel('Actual')\nplt.xlabel('Predicted')\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6761c666-e63d-4cfc-81f7-f56ef0f9bb42",
   "metadata": {},
   "outputs": [],
   "source": "# Multi-metric comparison: Baseline vs Tuned\ncomparison_df = pd.DataFrame({\n    'Model': ['DT Baseline', 'DT Tuned'],\n    'Accuracy': [baseline_acc, tuned_acc],\n    'F1 (Up)': [baseline_f1, tuned_f1],\n    'ROC-AUC': [baseline_roc, tuned_roc],\n    'PR-AUC': [baseline_pr, tuned_pr]\n})\n\nprint(\"=\"*70)\nprint(\"MODEL COMPARISON: Baseline vs Tuned\")\nprint(\"=\"*70)\ndisplay(comparison_df)\n\n# Visualize comparison - all 4 metrics\nfig, axes = plt.subplots(1, 4, figsize=(16, 4))\nmetrics = ['Accuracy', 'F1 (Up)', 'ROC-AUC', 'PR-AUC']\ncolors = ['#3498db', '#2ecc71']\n\nfor i, metric in enumerate(metrics):\n    ax = axes[i]\n    values = comparison_df[metric].values\n    bars = ax.bar(['Baseline', 'Tuned'], values, color=colors)\n    ax.set_title(metric, fontsize=12, fontweight='bold')\n    ax.set_ylim(0, max(values) * 1.2)\n    \n    for bar, val in zip(bars, values):\n        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n               f'{val:.3f}', ha='center', va='bottom', fontsize=10)\n\nplt.suptitle('Decision Tree: Baseline vs Tuned Performance', fontsize=14, fontweight='bold', y=1.02)\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf77130d-93d9-4bf9-bd9a-4b104431ec72",
   "metadata": {},
   "outputs": [],
   "source": "# 6.1 Feature importances (including engineered features)\n\nimportances = dt_best.feature_importances_\nfeat_imp = pd.Series(importances, index=X.columns).sort_values(ascending=False)\n\nprint(\"Top 15 features by importance:\")\ndisplay(feat_imp.head(15))\n\n# Highlight engineered features\nengineered = ['curve_spread', 'storage_bcf_change_7d', 'HDD_total', 'CDD_total', \n              'net_weather', 'ret_1', 'ret_3', 'ret_5', 'ret_10']\ncolors = ['#e74c3c' if f in engineered else '#3498db' for f in feat_imp.head(15).index]\n\nplt.figure(figsize=(10, 6))\nfeat_imp.head(15).plot(kind=\"bar\", color=colors)\nplt.title(\"Top 15 Feature Importances (Tuned Decision Tree)\\nRed = Engineered Features\", fontsize=12)\nplt.ylabel(\"Importance\")\nplt.tight_layout()\nplt.show()\n\n# Show engineered features importance\nprint(\"\\nEngineered Features Importance:\")\nfor f in engineered:\n    if f in feat_imp.index:\n        print(f\"  {f}: {feat_imp[f]:.4f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1df61e2-4d51-4cce-8d48-bc9e697551f3",
   "metadata": {},
   "outputs": [],
   "source": "# 6.2 Ablation Study: Remove redundant features and re-evaluate\n\n# Features to remove based on redundancy hypotheses\ncols_to_remove = [\n    'CDD_PA', 'CDD_IL', 'HDD_PA', 'HDD_IL',  # Regional weather redundancy\n    'contract_1_price', 'contract_2_price'    # Price levels (keep curve_spread)\n]\n\nprint(f\"Removing features: {cols_to_remove}\")\n\nX_reduced = X.drop(columns=[c for c in cols_to_remove if c in X.columns])\n\nprint(f\"Original features: {X.shape[1]}\")\nprint(f\"Reduced features: {X_reduced.shape[1]}\")\nprint(f\"\\nRemaining features: {list(X_reduced.columns)}\")\n\n# Apply same chronological split\nX_train_r, X_test_r = X_reduced[mask_train], X_reduced[mask_test]\n\n# Use best params from tuned model\nbest_params = grid_dt.best_params_\ndt_reduced = DecisionTreeClassifier(\n    random_state=RANDOM_STATE,\n    **best_params\n)\ndt_reduced.fit(X_train_r, y_train)\n\ny_pred_reduced = dt_reduced.predict(X_test_r)\ny_probs_reduced = dt_reduced.predict_proba(X_test_r)[:, 1]\n\n# Multi-metric evaluation\nreduced_acc = accuracy_score(y_test, y_pred_reduced)\nreduced_f1 = f1_score(y_test, y_pred_reduced, pos_label=1)\nreduced_roc = roc_auc_score(y_test, y_probs_reduced)\nreduced_pr = average_precision_score(y_test, y_probs_reduced)\n\nprint(\"\\n=== Decision Tree (Reduced Features) Results ===\")\nprint(f\"Accuracy:          {reduced_acc:.4f}\")\nprint(f\"F1 Score (Up):     {reduced_f1:.4f}\")\nprint(f\"ROC-AUC Score:     {reduced_roc:.4f}\")\nprint(f\"PR-AUC Score:      {reduced_pr:.4f}\")\n\nprint(\"\\nClassification report (reduced features):\")\nprint(classification_report(y_test, y_pred_reduced, target_names=['Down (0)', 'Up (1)']))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec8f367-fac0-4a8f-bace-456f888d1e4a",
   "metadata": {},
   "outputs": [],
   "source": "# 6.3 Final Model Comparison: All 3 variants\n\nfinal_comparison = pd.DataFrame({\n    'Model': ['DT Baseline', 'DT Tuned', 'DT Reduced Features'],\n    'Features': [X.shape[1], X.shape[1], X_reduced.shape[1]],\n    'Accuracy': [baseline_acc, tuned_acc, reduced_acc],\n    'F1 (Up)': [baseline_f1, tuned_f1, reduced_f1],\n    'ROC-AUC': [baseline_roc, tuned_roc, reduced_roc],\n    'PR-AUC': [baseline_pr, tuned_pr, reduced_pr]\n})\n\nprint(\"=\"*80)\nprint(\"FINAL MODEL COMPARISON (ALL VARIANTS)\")\nprint(\"=\"*80)\ndisplay(final_comparison)\n\n# Visualize - All 3 models, all 4 metrics\nfig, axes = plt.subplots(1, 4, figsize=(18, 5))\nmetrics = ['Accuracy', 'F1 (Up)', 'ROC-AUC', 'PR-AUC']\ncolors = ['#3498db', '#e74c3c', '#2ecc71']\n\nfor i, metric in enumerate(metrics):\n    ax = axes[i]\n    values = final_comparison[metric].values\n    bars = ax.bar(['Baseline', 'Tuned', 'Reduced'], values, color=colors)\n    ax.set_title(metric, fontsize=12, fontweight='bold')\n    ax.set_ylim(0, max(values) * 1.2)\n    \n    for bar, val in zip(bars, values):\n        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n               f'{val:.3f}', ha='center', va='bottom', fontsize=9)\n\nplt.suptitle('Decision Tree Model Performance Comparison (Binary Classification)', \n             fontsize=14, fontweight='bold', y=1.02)\nplt.tight_layout()\nplt.show()\n\n# Print best parameters\nprint(\"\\n\" + \"=\"*80)\nprint(\"BEST GRIDSEARCHCV PARAMETERS\")\nprint(\"=\"*80)\nfor param, value in grid_dt.best_params_.items():\n    print(f\"  {param}: {value}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac70966b-c82f-4abb-b222-e3c34b8ee6ca",
   "metadata": {},
   "outputs": [],
   "source": "# 7. Visualize a shallow decision tree for presentation\n\ndt_for_plot = DecisionTreeClassifier(\n    random_state=RANDOM_STATE,\n    class_weight=\"balanced\",\n    max_depth=3,          # small depth so it fits on slide\n    min_samples_leaf=10\n)\ndt_for_plot.fit(X_train, y_train)\n\nplt.figure(figsize=(20, 10))\nplot_tree(\n    dt_for_plot,\n    feature_names=X.columns,\n    class_names=[\"Down (0)\", \"Up (1)\"],  # Binary classes\n    filled=True,\n    rounded=True,\n    fontsize=9\n)\nplt.title(\"Simplified Decision Tree (max_depth=3) - Binary Classification\", fontsize=14)\nplt.tight_layout()\nplt.show()\n\n# Summary\nprint(\"\\n\" + \"=\"*80)\nprint(\"SUMMARY\")\nprint(\"=\"*80)\nprint(f\"\"\"\nThis enhanced Decision Tree notebook incorporates:\n1. Binary target (PriceUp) for next-day direction prediction\n2. Engineered features: curve_spread, storage_change, momentum indicators (ret_1/3/5/10)\n3. Chronological train/test split at {SPLIT_DATE}\n4. TimeSeriesSplit cross-validation (proper for time-series)\n5. GridSearchCV optimizing F1 score (for minority class)\n6. Multiple evaluation metrics: Accuracy, F1, ROC-AUC, PR-AUC\n7. Ablation study with reduced features\n\nBest performing model: {'Tuned' if tuned_f1 >= max(baseline_f1, reduced_f1) else 'Reduced' if reduced_f1 > tuned_f1 else 'Baseline'} Decision Tree\nBest F1 Score (Up class): {max(baseline_f1, tuned_f1, reduced_f1):.4f}\n\"\"\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
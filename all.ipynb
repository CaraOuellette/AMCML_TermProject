{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6f359169",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    classification_report,\n",
    "    confusion_matrix\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "RANDOM_STATE = 42\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ab72e50d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5202, 21),\n",
       "                   date  CDD_TX  CDD_PA  CDD_IL  CDD_NY  HDD_TX  HDD_PA  \\\n",
       " 0  2010-01-01 00:00:00       0       0       0       0      22      33   \n",
       " 1  2010-01-02 00:00:00       0       0       0       0      24      43   \n",
       " 2  2010-01-03 00:00:00       0       0       0       0      24      48   \n",
       " 3  2010-01-04 00:00:00       0       0       0       0      26      45   \n",
       " 4  2010-01-05 00:00:00       0       0       0       0      28      42   \n",
       " \n",
       "    HDD_IL  HDD_NY  contract_1_price  ...  spot_price  storage_bcf  \\\n",
       " 0      56      33              5.88  ...        6.09         3117   \n",
       " 1      61      39              5.88  ...        6.09         3117   \n",
       " 2      60      49              5.88  ...        6.09         3117   \n",
       " 3      58      45              5.88  ...        6.09         3117   \n",
       " 4      54      44              5.64  ...        6.19         3117   \n",
       " \n",
       "    us_gas_rigs  year  month  day_of_year  day_of_week  quarter  \\\n",
       " 0          759  2010      1            1            4        1   \n",
       " 1          759  2010      1            2            5        1   \n",
       " 2          759  2010      1            3            6        1   \n",
       " 3          759  2010      1            4            0        1   \n",
       " 4          759  2010      1            5            1        1   \n",
       " \n",
       "    price_movement_raw  price_movement_scaled  \n",
       " 0                 0.0                      0  \n",
       " 1                 0.0                      0  \n",
       " 2                 0.0                      0  \n",
       " 3                 0.0                      0  \n",
       " 4                 0.1                      1  \n",
       " \n",
       " [5 rows x 21 columns])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"master_dataset_ml_ready_labelled.csv\")\n",
    "\n",
    "df.shape, df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "14f6890e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.DataFrame'>\n",
      "RangeIndex: 5202 entries, 0 to 5201\n",
      "Data columns (total 21 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   date                   5202 non-null   str    \n",
      " 1   CDD_TX                 5202 non-null   int64  \n",
      " 2   CDD_PA                 5202 non-null   int64  \n",
      " 3   CDD_IL                 5202 non-null   int64  \n",
      " 4   CDD_NY                 5202 non-null   int64  \n",
      " 5   HDD_TX                 5202 non-null   int64  \n",
      " 6   HDD_PA                 5202 non-null   int64  \n",
      " 7   HDD_IL                 5202 non-null   int64  \n",
      " 8   HDD_NY                 5202 non-null   int64  \n",
      " 9   contract_1_price       5202 non-null   float64\n",
      " 10  contract_2_price       5202 non-null   float64\n",
      " 11  spot_price             5202 non-null   float64\n",
      " 12  storage_bcf            5202 non-null   int64  \n",
      " 13  us_gas_rigs            5202 non-null   int64  \n",
      " 14  year                   5202 non-null   int64  \n",
      " 15  month                  5202 non-null   int64  \n",
      " 16  day_of_year            5202 non-null   int64  \n",
      " 17  day_of_week            5202 non-null   int64  \n",
      " 18  quarter                5202 non-null   int64  \n",
      " 19  price_movement_raw     5202 non-null   float64\n",
      " 20  price_movement_scaled  5202 non-null   int64  \n",
      "dtypes: float64(4), int64(16), str(1)\n",
      "memory usage: 853.6 KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.info()\n",
    "df.isnull().sum()\n",
    "df.duplicated().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "049a44b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (5202, 18)\n",
      "y shape: (5202,)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CDD_TX</th>\n",
       "      <th>CDD_PA</th>\n",
       "      <th>CDD_IL</th>\n",
       "      <th>CDD_NY</th>\n",
       "      <th>HDD_TX</th>\n",
       "      <th>HDD_PA</th>\n",
       "      <th>HDD_IL</th>\n",
       "      <th>HDD_NY</th>\n",
       "      <th>contract_1_price</th>\n",
       "      <th>contract_2_price</th>\n",
       "      <th>spot_price</th>\n",
       "      <th>storage_bcf</th>\n",
       "      <th>us_gas_rigs</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day_of_year</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>quarter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>33</td>\n",
       "      <td>56</td>\n",
       "      <td>33</td>\n",
       "      <td>5.88</td>\n",
       "      <td>5.84</td>\n",
       "      <td>6.09</td>\n",
       "      <td>3117</td>\n",
       "      <td>759</td>\n",
       "      <td>2010</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>43</td>\n",
       "      <td>61</td>\n",
       "      <td>39</td>\n",
       "      <td>5.88</td>\n",
       "      <td>5.84</td>\n",
       "      <td>6.09</td>\n",
       "      <td>3117</td>\n",
       "      <td>759</td>\n",
       "      <td>2010</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>48</td>\n",
       "      <td>60</td>\n",
       "      <td>49</td>\n",
       "      <td>5.88</td>\n",
       "      <td>5.84</td>\n",
       "      <td>6.09</td>\n",
       "      <td>3117</td>\n",
       "      <td>759</td>\n",
       "      <td>2010</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>45</td>\n",
       "      <td>58</td>\n",
       "      <td>45</td>\n",
       "      <td>5.88</td>\n",
       "      <td>5.84</td>\n",
       "      <td>6.09</td>\n",
       "      <td>3117</td>\n",
       "      <td>759</td>\n",
       "      <td>2010</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>42</td>\n",
       "      <td>54</td>\n",
       "      <td>44</td>\n",
       "      <td>5.64</td>\n",
       "      <td>5.59</td>\n",
       "      <td>6.19</td>\n",
       "      <td>3117</td>\n",
       "      <td>759</td>\n",
       "      <td>2010</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CDD_TX  CDD_PA  CDD_IL  CDD_NY  HDD_TX  HDD_PA  HDD_IL  HDD_NY  \\\n",
       "0       0       0       0       0      22      33      56      33   \n",
       "1       0       0       0       0      24      43      61      39   \n",
       "2       0       0       0       0      24      48      60      49   \n",
       "3       0       0       0       0      26      45      58      45   \n",
       "4       0       0       0       0      28      42      54      44   \n",
       "\n",
       "   contract_1_price  contract_2_price  spot_price  storage_bcf  us_gas_rigs  \\\n",
       "0              5.88              5.84        6.09         3117          759   \n",
       "1              5.88              5.84        6.09         3117          759   \n",
       "2              5.88              5.84        6.09         3117          759   \n",
       "3              5.88              5.84        6.09         3117          759   \n",
       "4              5.64              5.59        6.19         3117          759   \n",
       "\n",
       "   year  month  day_of_year  day_of_week  quarter  \n",
       "0  2010      1            1            4        1  \n",
       "1  2010      1            2            5        1  \n",
       "2  2010      1            3            6        1  \n",
       "3  2010      1            4            0        1  \n",
       "4  2010      1            5            1        1  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Target variable (classification label)\n",
    "# This is the variable we want to predict\n",
    "y = df[\"price_movement_scaled\"]\n",
    "\n",
    "# Feature matrix\n",
    "# We drop:\n",
    "# 1. price_movement_scaled -> target variable (prevents data leakage)\n",
    "# 2. price_movement_raw    -> continuous form of target (prevents label leakage)\n",
    "# 3. date                  -> non-numeric and redundant (time features already exist)\n",
    "X = df.drop(\n",
    "    columns=[\"price_movement_scaled\", \"price_movement_raw\", \"date\"],\n",
    "    errors=\"ignore\"\n",
    ")\n",
    "\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6883e2fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "price_movement_scaled\n",
       " 0    0.393310\n",
       "-1    0.306613\n",
       " 1    0.300077\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts(normalize=True)\n",
    "# Check class distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ed1340e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (4161, 18) (4161,)\n",
      "Test shape: (1041, 18) (1041,)\n",
      "\n",
      "Train class distribution:\n",
      "price_movement_scaled\n",
      " 0    0.393175\n",
      "-1    0.306657\n",
      " 1    0.300168\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Test class distribution:\n",
      "price_movement_scaled\n",
      " 0    0.393852\n",
      "-1    0.306436\n",
      " 1    0.299712\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,          # 80% train, 20% test\n",
    "    stratify=y,             # preserve class proportions\n",
    "    random_state=42         # reproducibility\n",
    ")\n",
    "\n",
    "print(\"Train shape:\", X_train.shape, y_train.shape)\n",
    "print(\"Test shape:\", X_test.shape, y_test.shape)\n",
    "\n",
    "print(\"\\nTrain class distribution:\")\n",
    "print(y_train.value_counts(normalize=True))\n",
    "\n",
    "print(\"\\nTest class distribution:\")\n",
    "print(y_test.value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bb3be857",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "baseline_models = {\n",
    "    \"Logistic Regression\": Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"model\", LogisticRegression(\n",
    "            max_iter=2000,\n",
    "            random_state=42\n",
    "        ))\n",
    "    ]),\n",
    "    \n",
    "    \"Decision Tree\": DecisionTreeClassifier(\n",
    "        random_state=42\n",
    "    ),\n",
    "    \n",
    "    \"Random Forest\": RandomForestClassifier(\n",
    "        random_state=42\n",
    "    ),\n",
    "    \n",
    "    \"SGD\": Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"model\", SGDClassifier(\n",
    "            random_state=42\n",
    "        ))\n",
    "    ]),\n",
    "    \n",
    "    \"SVM\": Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"model\", SVC(\n",
    "            random_state=42\n",
    "        ))\n",
    "    ])\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "efa63468",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1_macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.562920</td>\n",
       "      <td>0.526316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.580211</td>\n",
       "      <td>0.560572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.603266</td>\n",
       "      <td>0.592001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SGD</td>\n",
       "      <td>0.540826</td>\n",
       "      <td>0.499043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SVM</td>\n",
       "      <td>0.588857</td>\n",
       "      <td>0.575513</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model  Accuracy  F1_macro\n",
       "0  Logistic Regression  0.562920  0.526316\n",
       "1        Decision Tree  0.580211  0.560572\n",
       "2        Random Forest  0.603266  0.592001\n",
       "3                  SGD  0.540826  0.499043\n",
       "4                  SVM  0.588857  0.575513"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "baseline_results = []\n",
    "\n",
    "for name, model in baseline_models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    \n",
    "    baseline_results.append({\n",
    "        \"Model\": name,\n",
    "        \"Accuracy\": accuracy_score(y_test, preds),\n",
    "        \"F1_macro\": f1_score(y_test, preds, average=\"macro\")\n",
    "    })\n",
    "\n",
    "baseline_results_df = pd.DataFrame(baseline_results)\n",
    "baseline_results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "04ee113d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'model__C': 0.1, 'model__penalty': 'l2', 'model__solver': 'lbfgs'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\programming\\assignments\\group_assignment_maths\\AMCML_TermProject\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "lr_pipeline = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"model\", LogisticRegression(\n",
    "        max_iter=3000,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "lr_param_grid = {\n",
    "    \"model__C\": [0.01, 0.1, 1, 10],\n",
    "    \"model__penalty\": [\"l2\"],\n",
    "    \"model__solver\": [\"lbfgs\"]\n",
    "}\n",
    "\n",
    "lr_grid = GridSearchCV(\n",
    "    lr_pipeline,\n",
    "    lr_param_grid,\n",
    "    cv=5,\n",
    "    scoring=\"f1_macro\",\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "lr_grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters:\", lr_grid.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "44a71e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Logistic Regression\n",
      "Accuracy: 0.5619596541786743\n",
      "F1_macro: 0.5240384681400494\n"
     ]
    }
   ],
   "source": [
    "best_lr = lr_grid.best_estimator_\n",
    "\n",
    "lr_preds = best_lr.predict(X_test)\n",
    "\n",
    "print(\"Tuned Logistic Regression\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, lr_preds))\n",
    "print(\"F1_macro:\", f1_score(y_test, lr_preds, average=\"macro\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6cfc51",
   "metadata": {},
   "source": [
    "### Logistic Regression: Baseline vs Tuned Comparison\n",
    "\n",
    "In this step, Logistic Regression was evaluated before and after hyperparameter tuning using GridSearchCV.\n",
    "\n",
    "The baseline Logistic Regression model achieved an accuracy of approximately 56% and a macro F1-score of approximately 0.53. After tuning the regularization parameter using GridSearchCV, the performance remained nearly unchanged.\n",
    "\n",
    "This indicates that Logistic Regression did not benefit significantly from hyperparameter tuning for this dataset. This suggests that the modelâ€™s linear decision boundary limits its ability to capture the underlying patterns in the data, rather than the choice of hyperparameters.\n",
    "\n",
    "Therefore, further performance improvements for this dataset are more likely to come from non-linear models rather than additional tuning of Logistic Regression.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "426d8705",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "dt_param_grid = {\n",
    "    \"max_depth\": [None, 5, 10, 20, 30],\n",
    "    \"min_samples_split\": [2, 10, 50],\n",
    "    \"min_samples_leaf\": [1, 5, 10],\n",
    "    \"criterion\": [\"gini\", \"entropy\"]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "31224f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Decision Tree parameters:\n",
      "{'criterion': 'gini', 'max_depth': 5, 'min_samples_leaf': 10, 'min_samples_split': 2}\n"
     ]
    }
   ],
   "source": [
    "dt_grid = GridSearchCV(\n",
    "    dt,\n",
    "    dt_param_grid,\n",
    "    cv=5,\n",
    "    scoring=\"f1_macro\",\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "dt_grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Decision Tree parameters:\")\n",
    "print(dt_grid.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2ed5b933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Decision Tree Results\n",
      "Accuracy: 0.633045148895293\n",
      "F1_macro: 0.6299798209052861\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "best_dt = dt_grid.best_estimator_\n",
    "\n",
    "dt_preds = best_dt.predict(X_test)\n",
    "\n",
    "print(\"Tuned Decision Tree Results\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, dt_preds))\n",
    "print(\"F1_macro:\", f1_score(y_test, dt_preds, average=\"macro\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba252f0",
   "metadata": {},
   "source": [
    "### Decision Tree: Baseline vs Tuned Comparison\n",
    "The Decision Tree classifier showed a significant improvement after hyperparameter tuning using GridSearchCV.\n",
    "\n",
    "The baseline Decision Tree achieved an accuracy of approximately 58% and a macro F1-score of approximately 0.56. After tuning parameters such as maximum depth, minimum samples per split, and minimum samples per leaf, the tuned model achieved an accuracy of approximately 63% and a macro F1-score of approximately 0.63.\n",
    "\n",
    "This improvement indicates that controlling the complexity of the Decision Tree helps reduce overfitting and allows the model to generalize better on unseen data. Unlike Logistic Regression, the Decision Tree benefits substantially from hyperparameter tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "51492b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf_param_grid = {\n",
    "    \"n_estimators\": [200, 500],\n",
    "    \"max_depth\": [None, 10, 20],\n",
    "    \"min_samples_split\": [2, 10],\n",
    "    \"min_samples_leaf\": [1, 5],\n",
    "    \"max_features\": [\"sqrt\", \"log2\"]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f4ea8894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Random Forest parameters:\n",
      "{'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 500}\n"
     ]
    }
   ],
   "source": [
    "rf_grid = GridSearchCV(\n",
    "    rf,\n",
    "    rf_param_grid,\n",
    "    cv=5,\n",
    "    scoring=\"f1_macro\",\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf_grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Random Forest parameters:\")\n",
    "print(rf_grid.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2e562c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Random Forest Results\n",
      "Accuracy: 0.6205571565802114\n",
      "F1_macro: 0.6173504342049523\n"
     ]
    }
   ],
   "source": [
    "best_rf = rf_grid.best_estimator_\n",
    "\n",
    "rf_preds = best_rf.predict(X_test)\n",
    "\n",
    "print(\"Tuned Random Forest Results\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, rf_preds))\n",
    "print(\"F1_macro:\", f1_score(y_test, rf_preds, average=\"macro\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83c8828",
   "metadata": {},
   "source": [
    "### Random Forest: Baseline vs Tuned Comparison\n",
    "The Random Forest classifier showed a moderate improvement after hyperparameter tuning using GridSearchCV.\n",
    "\n",
    "The baseline Random Forest achieved an accuracy of approximately 60% and a macro F1-score of approximately 0.59. After tuning the number of trees, maximum depth, minimum samples per split, and feature selection strategy, the tuned model achieved an accuracy of approximately 62% and a macro F1-score of approximately 0.62.\n",
    "\n",
    "While the tuned Random Forest improved upon its baseline performance, the improvement was less pronounced compared to the Decision Tree. This suggests that Random Forest was already relatively well-configured by default, and hyperparameter tuning provided incremental gains rather than dramatic improvements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d788ff9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "sgd_pipeline = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"model\", SGDClassifier(\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "sgd_param_grid = {\n",
    "    \"model__loss\": [\"hinge\", \"log_loss\"],\n",
    "    \"model__alpha\": [1e-4, 1e-3, 1e-2],\n",
    "    \"model__penalty\": [\"l2\", \"l1\", \"elasticnet\"]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bf393167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best SGD parameters:\n",
      "{'model__alpha': 0.0001, 'model__loss': 'log_loss', 'model__penalty': 'elasticnet'}\n"
     ]
    }
   ],
   "source": [
    "sgd_grid = GridSearchCV(\n",
    "    sgd_pipeline,\n",
    "    sgd_param_grid,\n",
    "    cv=5,\n",
    "    scoring=\"f1_macro\",\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "sgd_grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best SGD parameters:\")\n",
    "print(sgd_grid.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e63c3121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned SGD Results\n",
      "Accuracy: 0.5667627281460135\n",
      "F1_macro: 0.5402883583600904\n"
     ]
    }
   ],
   "source": [
    "best_sgd = sgd_grid.best_estimator_\n",
    "\n",
    "sgd_preds = best_sgd.predict(X_test)\n",
    "\n",
    "print(\"Tuned SGD Results\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, sgd_preds))\n",
    "print(\"F1_macro:\", f1_score(y_test, sgd_preds, average=\"macro\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8f5dc8",
   "metadata": {},
   "source": [
    "### SGD Classifier: Baseline vs Tuned Comparison\n",
    "The SGD classifier showed an improvement in performance after hyperparameter tuning using GridSearchCV.\n",
    "\n",
    "The baseline SGD model achieved an accuracy of approximately 54% and a macro F1-score of approximately 0.50. After tuning the loss function, regularization strength, and penalty type, the tuned model achieved an accuracy of approximately 57% and a macro F1-score of approximately 0.54.\n",
    "\n",
    "Although hyperparameter tuning improved the performance of the SGD classifier, its overall performance remained lower than tree-based models. This suggests that linear models optimized via stochastic gradient descent may be insufficient to fully capture the non-linear relationships present in the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2f4c5712",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_pipeline = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"model\", SVC(random_state=42))\n",
    "])\n",
    "\n",
    "svm_param_grid = {\n",
    "    \"model__C\": [0.1, 1, 10],\n",
    "    \"model__kernel\": [\"rbf\", \"linear\"],\n",
    "    \"model__gamma\": [\"scale\", \"auto\"]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2262a374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best SVM parameters:\n",
      "{'model__C': 10, 'model__gamma': 'scale', 'model__kernel': 'rbf'}\n"
     ]
    }
   ],
   "source": [
    "svm_grid = GridSearchCV(\n",
    "    svm_pipeline,\n",
    "    svm_param_grid,\n",
    "    cv=5,\n",
    "    scoring=\"f1_macro\",\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "svm_grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best SVM parameters:\")\n",
    "print(svm_grid.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "330cbdcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned SVM Results\n",
      "Accuracy: 0.6013448607108549\n",
      "F1_macro: 0.5922613559530395\n"
     ]
    }
   ],
   "source": [
    "best_svm = svm_grid.best_estimator_\n",
    "\n",
    "svm_preds = best_svm.predict(X_test)\n",
    "\n",
    "print(\"Tuned SVM Results\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, svm_preds))\n",
    "print(\"F1_macro:\", f1_score(y_test, svm_preds, average=\"macro\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c9f71a",
   "metadata": {},
   "source": [
    "### Support Vector Machine: Baseline vs Tuned Comparison\n",
    "The Support Vector Machine (SVM) classifier showed a moderate improvement in performance after hyperparameter tuning using GridSearchCV.\n",
    "\n",
    "The baseline SVM achieved an accuracy of approximately 59% and a macro F1-score of approximately 0.58. After tuning the regularization parameter and kernel type, the tuned SVM achieved an accuracy of approximately 60% and a macro F1-score of approximately 0.59.\n",
    "\n",
    "This improvement indicates that SVM benefits from hyperparameter tuning; however, its performance remains slightly lower than tree-based models. This suggests that while SVM can model complex decision boundaries, ensemble and tree-based approaches are more effective for this dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c1a4e6",
   "metadata": {},
   "source": [
    "### Summary of GridSearchCV Results\n",
    "GridSearchCV was applied to all five classifiers to evaluate whether hyperparameter tuning could improve model performance.\n",
    "\n",
    "Among the models tested, Decision Tree benefited the most from tuning, achieving the highest macro F1-score. Random Forest and SVM showed moderate improvements after tuning, while SGD showed limited improvement. Logistic Regression did not significantly benefit from hyperparameter tuning, indicating that model complexity rather than hyperparameter choice limited its performance.\n",
    "\n",
    "Overall, tree-based models demonstrated superior performance compared to linear models for this classification task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5c3ca7",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f86665",
   "metadata": {},
   "source": [
    "### Random Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "743cd1cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomly removed features:\n",
      "['CDD_TX', 'CDD_PA', 'contract_1_price', 'HDD_PA', 'CDD_NY']\n",
      "New feature count: 13\n"
     ]
    }
   ],
   "source": [
    "# Set seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Number of features to remove\n",
    "num_features_to_remove = int(0.3 * X_train.shape[1])\n",
    "\n",
    "# Randomly choose features to drop\n",
    "random_features_to_drop = np.random.choice(\n",
    "    X_train.columns,\n",
    "    size=num_features_to_remove,\n",
    "    replace=False\n",
    ")\n",
    "\n",
    "print(\"Randomly removed features:\")\n",
    "print(list(random_features_to_drop))\n",
    "\n",
    "# Reduced feature sets\n",
    "X_train_rand = X_train.drop(columns=random_features_to_drop)\n",
    "X_test_rand = X_test.drop(columns=random_features_to_drop)\n",
    "\n",
    "print(\"New feature count:\", X_train_rand.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "27cbb69e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1_macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.634966</td>\n",
       "      <td>0.631746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.614793</td>\n",
       "      <td>0.611068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SVM</td>\n",
       "      <td>0.609030</td>\n",
       "      <td>0.601504</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Model  Accuracy  F1_macro\n",
       "0  Decision Tree  0.634966  0.631746\n",
       "1  Random Forest  0.614793  0.611068\n",
       "2            SVM  0.609030  0.601504"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_removal_results = []\n",
    "\n",
    "# Tuned models from earlier\n",
    "models_reduced = {\n",
    "    \"Decision Tree\": best_dt,\n",
    "    \"Random Forest\": best_rf,\n",
    "    \"SVM\": best_svm\n",
    "}\n",
    "\n",
    "for name, model in models_reduced.items():\n",
    "    model.fit(X_train_rand, y_train)\n",
    "    preds = model.predict(X_test_rand)\n",
    "\n",
    "    random_removal_results.append({\n",
    "        \"Model\": name,\n",
    "        \"Accuracy\": accuracy_score(y_test, preds),\n",
    "        \"F1_macro\": f1_score(y_test, preds, average=\"macro\")\n",
    "    })\n",
    "\n",
    "pd.DataFrame(random_removal_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d387cd9",
   "metadata": {},
   "source": [
    "### Random Feature Removal Experiment\n",
    "In this experiment, approximately 30% of the input features were randomly removed to evaluate model robustness and sensitivity to feature reduction.\n",
    "\n",
    "The results show that random feature removal did not significantly degrade model performance. The Decision Tree and SVM models showed slight improvements in macro F1-score, while Random Forest experienced a small decrease in performance.\n",
    "\n",
    "These results suggest that the dataset contains redundant or weakly informative features, and that some models can benefit from a reduced feature space. Random Forest, however, relies more on feature diversity and was slightly more sensitive to random feature removal.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d28f540",
   "metadata": {},
   "source": [
    "### Hypothesis-Based Feature Removal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542557b9",
   "metadata": {},
   "source": [
    "Weather-related features (CDD/HDD) play a significant role in predicting price movement.\n",
    "Removing them should negatively impact model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "24160880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining feature count: 10\n"
     ]
    }
   ],
   "source": [
    "weather_features = [\n",
    "    \"CDD_TX\", \"CDD_PA\", \"CDD_IL\", \"CDD_NY\",\n",
    "    \"HDD_TX\", \"HDD_PA\", \"HDD_IL\", \"HDD_NY\"\n",
    "]\n",
    "\n",
    "X_train_hyp = X_train.drop(columns=weather_features)\n",
    "X_test_hyp = X_test.drop(columns=weather_features)\n",
    "\n",
    "print(\"Remaining feature count:\", X_train_hyp.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6a7384eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1_macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.637848</td>\n",
       "      <td>0.633057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.619597</td>\n",
       "      <td>0.616159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SVM</td>\n",
       "      <td>0.617675</td>\n",
       "      <td>0.614479</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Model  Accuracy  F1_macro\n",
       "0  Decision Tree  0.637848  0.633057\n",
       "1  Random Forest  0.619597  0.616159\n",
       "2            SVM  0.617675  0.614479"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypothesis_results = []\n",
    "\n",
    "for name, model in models_reduced.items():\n",
    "    model.fit(X_train_hyp, y_train)\n",
    "    preds = model.predict(X_test_hyp)\n",
    "\n",
    "    hypothesis_results.append({\n",
    "        \"Model\": name,\n",
    "        \"Accuracy\": accuracy_score(y_test, preds),\n",
    "        \"F1_macro\": f1_score(y_test, preds, average=\"macro\")\n",
    "    })\n",
    "\n",
    "pd.DataFrame(hypothesis_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e583d05",
   "metadata": {},
   "source": [
    "In this experiment, weather-related features (CDD and HDD variables) were removed based on the hypothesis that they significantly influence price movement.\n",
    "\n",
    "The results show that removing weather features did not reduce model performance. In fact, Decision Tree and SVM models showed slight improvements in macro F1-score, while Random Forest experienced only a marginal decrease.\n",
    "\n",
    "These findings suggest that weather-related variables are not dominant predictors in this dataset and that price movement can be effectively modeled using price, storage, supply, and time-based features. This highlights the presence of redundant or weakly informative features in the weather group.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "225c3aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\programming\\assignments\\group_assignment_maths\\AMCML_TermProject\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features:\n",
      "['spot_price', 'day_of_week', 'HDD_PA', 'contract_2_price', 'HDD_NY', 'contract_1_price', 'us_gas_rigs', 'CDD_TX']\n"
     ]
    }
   ],
   "source": [
    "from utils.auto_feature_selector import autoFeatureSelector\n",
    "\n",
    "selected_features = autoFeatureSelector(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    num_feats=8,\n",
    "    methods=[\"pearson\", \"chi-square\", \"rfe\", \"log-reg\", \"rf\"]\n",
    ")\n",
    "\n",
    "print(\"Selected features:\")\n",
    "print(selected_features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8ba1bddc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1_macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.634006</td>\n",
       "      <td>0.631781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.613833</td>\n",
       "      <td>0.610991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SVM</td>\n",
       "      <td>0.621518</td>\n",
       "      <td>0.618624</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Model  Accuracy  F1_macro\n",
       "0  Decision Tree  0.634006  0.631781\n",
       "1  Random Forest  0.613833  0.610991\n",
       "2            SVM  0.621518  0.618624"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_auto = X_train[selected_features]\n",
    "X_test_auto = X_test[selected_features]\n",
    "\n",
    "auto_fs_results = []\n",
    "\n",
    "for name, model in models_reduced.items():\n",
    "    model.fit(X_train_auto, y_train)\n",
    "    preds = model.predict(X_test_auto)\n",
    "\n",
    "    auto_fs_results.append({\n",
    "        \"Model\": name,\n",
    "        \"Accuracy\": accuracy_score(y_test, preds),\n",
    "        \"F1_macro\": f1_score(y_test, preds, average=\"macro\")\n",
    "    })\n",
    "\n",
    "pd.DataFrame(auto_fs_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fc82d3",
   "metadata": {},
   "source": [
    "### Final Conclusion\n",
    "\n",
    "This project evaluated the impact of feature selection on multiclass price movement prediction using multiple classification models. Experiments included baseline modeling with all features, random feature removal, hypothesis-driven feature removal, and automated feature selection.\n",
    "\n",
    "The results demonstrate that reducing the feature set does not degrade predictive performance and, in several cases, improves model generalization. Decision Tree models showed consistent performance across all experiments, while Support Vector Machines benefited the most from automated feature selection. Random Forest models exhibited slight sensitivity to feature reduction due to their reliance on feature diversity.\n",
    "\n",
    "Overall, the findings indicate that the dataset contains redundant or weakly informative features and that intelligent feature selection can simplify models while maintaining or improving predictive performance. The Decision Tree classifier emerged as the most robust and interpretable model for this task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff46aea",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

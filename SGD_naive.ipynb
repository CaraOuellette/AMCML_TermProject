{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5g9CdO3fZb47",
        "outputId": "9f7d33bf-27a9-4a3c-e267-ddf30ef15504"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape: (5202, 21)\n",
            "                  date  CDD_TX  CDD_PA  CDD_IL  CDD_NY  HDD_TX  HDD_PA  \\\n",
            "0  2010-01-01 00:00:00       0       0       0       0      22      33   \n",
            "1  2010-01-02 00:00:00       0       0       0       0      24      43   \n",
            "2  2010-01-03 00:00:00       0       0       0       0      24      48   \n",
            "\n",
            "   HDD_IL  HDD_NY  contract_1_price  ...  spot_price  storage_bcf  \\\n",
            "0      56      33              5.88  ...        6.09         3117   \n",
            "1      61      39              5.88  ...        6.09         3117   \n",
            "2      60      49              5.88  ...        6.09         3117   \n",
            "\n",
            "   us_gas_rigs  year  month  day_of_year  day_of_week  quarter  \\\n",
            "0          759  2010      1            1            4        1   \n",
            "1          759  2010      1            2            5        1   \n",
            "2          759  2010      1            3            6        1   \n",
            "\n",
            "   price_movement_raw  price_movement_scaled  \n",
            "0                 0.0                      0  \n",
            "1                 0.0                      0  \n",
            "2                 0.0                      0  \n",
            "\n",
            "[3 rows x 21 columns]\n"
          ]
        }
      ],
      "source": [
        "# Baseline: Logistic Regression (SGD-style workflow) for 3-class price movement\n",
        "# Dataset: master_dataset_ml_ready_labelled.csv\n",
        "# Target: price_movement_scaled in {-1, 0, 1}\n",
        "\n",
        "# --- 0) Imports ---\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn.metrics import accuracy_score, balanced_accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "\n",
        "# --- 1) Load data ---\n",
        "DATA_PATH = \"/content/master_dataset_ml_ready_labelled.csv\"\n",
        "\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "print(\"Shape:\", df.shape)\n",
        "print(df.head(3))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3201c98c"
      },
      "source": [
        "#plotting values\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Make sure date is datetime\n",
        "df_plot = df.copy()\n",
        "df_plot[\"date\"] = pd.to_datetime(df_plot[\"date\"], errors=\"coerce\")\n",
        "\n",
        "# Try to find a raw delta-like column automatically\n",
        "candidates = [c for c in df_plot.columns if any(tok in c.lower() for tok in [\"delta\", \"diff\", \"change\", \"return\"])]\n",
        "print(\"Delta-like columns found:\", candidates)\n",
        "\n",
        "# Pick the first one if it exists, else fall back to scaled target\n",
        "if candidates:\n",
        "    ycol = candidates[0]\n",
        "    title = f\"Date vs {ycol} (raw movement)\"\n",
        "else:\n",
        "    ycol = \"price_movement_raw\"\n",
        "    title = \"Date vs price_movement_raw\"\n",
        "\n",
        "# Drop rows with missing values for clean plotting\n",
        "plot_df = df_plot[[\"date\", ycol]].dropna().sort_values(\"date\")\n",
        "plot_df = plot_df[(plot_df[\"date\"] >= \"2024-01-01\") & (plot_df[\"date\"] <= \"2024-12-31\")]\n",
        "\n",
        "plt.figure(figsize=(14, 5))\n",
        "plt.scatter(plot_df[\"date\"], plot_df[ycol], s=3, alpha=0.4)\n",
        "plt.axhline(0, color=\"black\", linewidth=1, alpha=0.5)\n",
        "plt.title(title)\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(ycol)\n",
        "plt.tight_layout()\n",
        "plt.plot(plot_df[\"date\"], plot_df[ycol], linewidth=1, alpha=0.8)\n",
        "\n",
        "plt.ylim(-0.5, 0.5)\n",
        "\n",
        "\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 2) Basic sanity checks ---\n",
        "TARGET_COL = \"price_movement_scaled\"\n",
        "DATE_COL = \"date\"  # per your description\n",
        "\n",
        "if TARGET_COL not in df.columns:\n",
        "    raise ValueError(f\"Target column '{TARGET_COL}' not found. Available columns: {list(df.columns)[:20]}...\")\n",
        "\n",
        "# Print target distribution\n",
        "print(\"\\nTarget distribution:\")\n",
        "print(df[TARGET_COL].value_counts(dropna=False).sort_index())\n",
        "\n",
        "# Peek at dtypes\n",
        "print(\"\\nDtypes (first 25):\")\n",
        "print(df.dtypes.head(25))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_fCprkUcZuNb",
        "outputId": "4cd13019-c4ec-4c83-8dc6-9ffefca5a94c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Target distribution:\n",
            "price_movement_scaled\n",
            "-1    1595\n",
            " 0    2046\n",
            " 1    1561\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Dtypes (first 25):\n",
            "date                      object\n",
            "CDD_TX                     int64\n",
            "CDD_PA                     int64\n",
            "CDD_IL                     int64\n",
            "CDD_NY                     int64\n",
            "HDD_TX                     int64\n",
            "HDD_PA                     int64\n",
            "HDD_IL                     int64\n",
            "HDD_NY                     int64\n",
            "contract_1_price         float64\n",
            "contract_2_price         float64\n",
            "spot_price               float64\n",
            "storage_bcf                int64\n",
            "us_gas_rigs                int64\n",
            "year                       int64\n",
            "month                      int64\n",
            "day_of_year                int64\n",
            "day_of_week                int64\n",
            "quarter                    int64\n",
            "price_movement_raw       float64\n",
            "price_movement_scaled      int64\n",
            "dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 3) Define features (drop date for baseline) ---\n",
        "# For the first pass, we drop the date index-like column.\n",
        "# We'll also drop the target from X.\n",
        "drop_cols = [TARGET_COL, \"price_movement_raw\"]\n",
        "if DATE_COL in df.columns:\n",
        "    drop_cols.append(DATE_COL)\n",
        "\n",
        "X = df.drop(columns=drop_cols)\n",
        "y = df[TARGET_COL]\n",
        "\n",
        "# If y was read as float because of NaNs, coerce to int where possible.\n",
        "# Keep NaNs so we can drop them cleanly.\n",
        "y = pd.to_numeric(y, errors=\"coerce\")\n",
        "\n",
        "# Drop rows with missing target\n",
        "mask = y.notna()\n",
        "X = X.loc[mask].copy()\n",
        "y = y.loc[mask].astype(int)\n",
        "\n",
        "# Confirm allowed labels\n",
        "allowed = {-1, 0, 1}\n",
        "labels_present = set(np.unique(y))\n",
        "if not labels_present.issubset(allowed):\n",
        "    raise ValueError(f\"Unexpected labels in y: {sorted(labels_present)}. Expected subset of {sorted(allowed)}\")\n",
        "\n",
        "print(\"\\nAfter dropping missing target:\")\n",
        "print(\"X shape:\", X.shape)\n",
        "print(\"y distribution:\")\n",
        "print(y.value_counts().sort_index())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_zszE5dcj54",
        "outputId": "de48dd27-7575-47e3-ca07-cd560245e75c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "After dropping missing target:\n",
            "X shape: (5202, 18)\n",
            "y distribution:\n",
            "price_movement_scaled\n",
            "-1    1595\n",
            " 0    2046\n",
            " 1    1561\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 4) Train/test split ---\n",
        "# Stratify to preserve class proportions.\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=y\n",
        ")\n",
        "\n",
        "print(\"\\nSplit sizes:\")\n",
        "print(\"Train:\", X_train.shape, \"Test:\", X_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IuqyqSzQcnUk",
        "outputId": "75bd6748-ec79-4a39-9ba3-649d9e3859d6"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Split sizes:\n",
            "Train: (4161, 18) Test: (1041, 18)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 5) Build preprocessing + model pipeline ---\n",
        "# This baseline assumes features are numeric. We'll:\n",
        "# - impute missing values with median\n",
        "# - standardize\n",
        "# - fit multinomial logistic regression\n",
        "\n",
        "# Identify numeric columns\n",
        "numeric_cols = X_train.columns.tolist()\n",
        "\n",
        "preprocess = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\n",
        "            \"num\",\n",
        "            Pipeline(steps=[\n",
        "                (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "                (\"scaler\", StandardScaler()),\n",
        "            ]),\n",
        "            numeric_cols,\n",
        "        )\n",
        "    ],\n",
        "    remainder=\"drop\",\n",
        "    verbose_feature_names_out=False,\n",
        ")\n",
        "\n",
        "# Multiclass logistic regression.\n",
        "# 'saga' works well with many features and supports multinomial.\n",
        "# Increase max_iter if you see ConvergenceWarning.\n",
        "clf = LogisticRegression(\n",
        "    multi_class=\"multinomial\",\n",
        "    solver=\"saga\",\n",
        "    max_iter=5000,\n",
        "    n_jobs=None,\n",
        "    class_weight=None,  # consider 'balanced' if classes are very skewed\n",
        "    random_state=42,\n",
        ")\n",
        "\n",
        "pipe = Pipeline(steps=[\n",
        "    (\"preprocess\", preprocess),\n",
        "    (\"clf\", clf),\n",
        "])"
      ],
      "metadata": {
        "id": "ZHjBVoIyctKu"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 6) Baseline: Dummy classifier ---\n",
        "# Always predicts most frequent class\n",
        "baseline = DummyClassifier(strategy=\"most_frequent\", random_state=42)\n",
        "baseline.fit(X_train, y_train)\n",
        "base_pred = baseline.predict(X_test)\n",
        "\n",
        "print(\"\\n=== Dummy baseline ===\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, base_pred))\n",
        "print(\"Balanced accuracy:\", balanced_accuracy_score(y_test, base_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nW0T8vDlcvhA",
        "outputId": "22fefa60-4dd2-4fd8-cbe3-090afa820d3e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Dummy baseline ===\n",
            "Accuracy: 0.39385206532180594\n",
            "Balanced accuracy: 0.3333333333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 7) Train model ---\n",
        "pipe.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "# --- 8) Evaluate ---\n",
        "y_pred = pipe.predict(X_test)\n",
        "\n",
        "print(\"\\n=== Logistic Regression baseline ===\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Balanced accuracy:\", balanced_accuracy_score(y_test, y_pred))\n",
        "print(\"\\nConfusion matrix (rows=true, cols=pred):\")\n",
        "print(confusion_matrix(y_test, y_pred, labels=[-1, 0, 1]))\n",
        "print(\"\\nClassification report:\")\n",
        "print(classification_report(y_test, y_pred, labels=[-1, 0, 1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jeTwSDWkczVf",
        "outputId": "4485b97b-a31f-4f0e-d328-567300924c2c"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Logistic Regression baseline ===\n",
            "Accuracy: 0.5629202689721422\n",
            "Balanced accuracy: 0.5399658093518481\n",
            "\n",
            "Confusion matrix (rows=true, cols=pred):\n",
            "[[ 89 103 127]\n",
            " [ 30 329  51]\n",
            " [ 75  69 168]]\n",
            "\n",
            "Classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.46      0.28      0.35       319\n",
            "           0       0.66      0.80      0.72       410\n",
            "           1       0.49      0.54      0.51       312\n",
            "\n",
            "    accuracy                           0.56      1041\n",
            "   macro avg       0.53      0.54      0.53      1041\n",
            "weighted avg       0.54      0.56      0.54      1041\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#okay now we'll do the SGD, referencing the thing...after we have a little snack blondie\n",
        "from sklearn.linear_model import SGDClassifier #import the classifier\n",
        "#we can reuse our x_train and so on from log regress\n",
        "SGD_model = SGDClassifier(max_iter=1000, tol=1e-3, random_state=42) # tol: the stopping criterion, this is ripped from our lab notebook\n",
        "SGD_model.fit(X_train, y_train)\n",
        "SGD_predictions = SGD_model.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, SGD_predictions))\n",
        "from sklearn.metrics import confusion_matrix\n",
        "print(confusion_matrix(y_test,SGD_predictions))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOHD7gq1kEWy",
        "outputId": "6e04882f-c1d3-4b8b-c548-8e983e3879f3"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.2987512007684918\n",
            "[[  0   0 319]\n",
            " [  0   0 410]\n",
            " [  1   0 311]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#sorry, did this thing fucking get 0 of the -1 and 1s right? did it even predict?\n",
        "np.unique(SGD_predictions, return_counts=True)\n",
        "X_train.isna().any().any()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U6kdRFRB5tYe",
        "outputId": "0daf5640-28f8-4b2e-f9ac-3ecf6ce5af8f"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.False_"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#try again with feature scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "pipe2 = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"SGD\", SGDClassifier(max_iter=1000, tol=1e-3, random_state=42, loss=\"log_loss\")),\n",
        "])\n",
        "pipe2.fit(X_train, y_train)\n",
        "sgd_pred = pipe2.predict(X_test)\n",
        "print(\"\\n=== SGD naive results ===\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, sgd_pred))\n",
        "print(\"Balanced accuracy:\", balanced_accuracy_score(y_test, sgd_pred))\n",
        "print(\"\\nConfusion matrix (rows=true, cols=pred):\")\n",
        "print(confusion_matrix(y_test, sgd_pred, labels=[-1, 0, 1]))\n",
        "print(\"\\nClassification report:\")\n",
        "print(classification_report(y_test, sgd_pred, labels=[-1, 0, 1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cg1bOqiS6g9h",
        "outputId": "f0561fea-254d-4217-e8a8-28bd8e4eef0b"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== SGD naive results ===\n",
            "Accuracy: 0.5408261287223823\n",
            "Balanced accuracy: 0.5164874794395401\n",
            "\n",
            "Confusion matrix (rows=true, cols=pred):\n",
            "[[ 75 123 121]\n",
            " [ 25 326  59]\n",
            " [ 75  75 162]]\n",
            "\n",
            "Classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.43      0.24      0.30       319\n",
            "           0       0.62      0.80      0.70       410\n",
            "           1       0.47      0.52      0.50       312\n",
            "\n",
            "    accuracy                           0.54      1041\n",
            "   macro avg       0.51      0.52      0.50      1041\n",
            "weighted avg       0.52      0.54      0.52      1041\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-3p5QmUK66aN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}